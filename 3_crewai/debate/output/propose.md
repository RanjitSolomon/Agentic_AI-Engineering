There needs to be strict laws to regulate LLMs because, without regulation, these powerful tools pose significant risks to society, including misinformation, ethical misuse, and data privacy violations. 

Firstly, LLMs have the capability to generate content that can easily mislead or misinform the public. In a world filled with quickly digestible information, the ability of LLMs to create convincing yet false narratives can exacerbate the spread of fake news, leading to social division, public manipulation, and a breakdown of trust in media. Strict laws can ensure that there are safeguards in place to monitor and control the output of these models, thereby minimizing their potential for harm.

Secondly, the ethical implications of LLMs are profound. These models can inadvertently perpetuate biases present in their training data, leading to discriminatory practices in hiring, law enforcement, and beyond. Regulations can enforce transparency in how LLMs are trained and implemented, requiring developers to address and rectify inherent biases. Moreover, clear guidelines can promote accountability, establishing the need for ethical oversight within AI development.

Lastly, data privacy is a critical concern. LLMs often rely on vast datasets, which, without appropriate regulations, can include sensitive, personal information. To protect individuals' rights, strict laws must be enacted to govern data usage, ensuring that LLMs do not infringe upon an individual's privacy or lead to unauthorized data exploitation.

In conclusion, the implementation of strict laws to regulate LLMs is not just a precaution; it is a necessity to ensure the responsible and ethical use of these transformative technologies. By doing so, we can harness their potential benefits while safeguarding society against their inherent risks.